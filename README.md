### ğŸ‘¨â€ğŸ« Santander Customer Satisfaction
kaggleì—ì„œ ì œê³µí•˜ëŠ” Binary Classification with a Bank Churn Datasetì„ EDAì™€ model í•™ìŠµì„ í†µí•´ ê³ ê° ì´íƒˆ í™•ë¥ ì„ ì˜ˆì¸¡í•˜ëŠ” í”„ë¡œì íŠ¸

---
### â²ï¸ ë¶„ì„ ê¸°ê°„
2024.09.10 - 2024.09.13

---

### ğŸ“ ì†Œê°œ
Binary Classification with a Bank Churn Datasetì€ ì´ì „ì— ì§„í–‰í–ˆë˜ Santander Customer Satisfaction dataì™€ ë¹„ìŠ·í•˜ê²Œ ê³ ê° ì´íƒˆì— ëŒ€í•œ ì˜ˆì¸¡ì„ í•˜ëŠ” ë¬¸ì œì´ë‹¤. í•˜ì§€ë§Œ featureê°€ ê°€ë ¤ì ¸ ìˆëŠ” ê²ƒì´ ì•„ë‹ˆê¸° ë•Œë¬¸ì— Feature Engineeringì„ ì§„í–‰í•  ë•Œ featureë“¤ì„ ìœ ìš©í•˜ê²Œ ì´ìš©í•  ìˆ˜ ìˆë‹¤.

ë”°ë¼ì„œ ì´ë²ˆì—” ë¶„ì„í•  Binary Classification with a Bank Churn Datasetì€ Santander Customer Satisfaction dataì™€ titanic dataë¥¼ ì„ì–´ ë†“ì€ ë°ì´í„°ë¼ê³  í•  ìˆ˜ ìˆë‹¤.

---

### í”„ë¡œì íŠ¸ ê°œìš”
##### ğŸ“Œ ëª©í‘œ
kaggleì—ì„œ ì—…ë¡œë“œí•œ overviewëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

![image](https://github.com/user-attachments/assets/c043d858-8be7-43d8-b19d-b0f2d9268575)

ì¦‰, ê³ ê°ì´ ê³„ì†í•´ì„œ ì„œë¹„ìŠ¤ë¥¼ ì´ìš©í• ì§€ ì•„ë‹ˆë©´ ê³„ì •ì„ ë‹«ê³  ì„œë¹„ìŠ¤ë¥¼ ì¤‘ë‹¨í• ì§€ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒìœ¼ë¡œ ì´ì§„ë¶„ë¥˜ ë¬¸ì œì´ë‹¤

##### ğŸ–¥ï¸ ë°ì´í„°ì…‹ (Data Set)
ì´ í”„ë¡œì íŠ¸ì—ì„œ ì‚¬ìš©í•œ ë°ì´í„°ì…‹ì€ Kaggleì—ì„œ ì œê³µí•˜ëŠ” ë‹¤ìŒ íŒŒì¼ë“¤ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
1. train.csv: í›ˆë ¨ ë°ì´í„°ì…‹, íŠ¹ì§•ë“¤ê³¼ ëª©í‘œ ë³€ìˆ˜ë¥¼ í¬í•¨.
2. test.csv: í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹, ì˜ˆì¸¡ì„ ìœ„í•´ ì‚¬ìš©ë  ë°ì´í„°.
3. sample_submission.csv: ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì œì¶œí•˜ê¸° ìœ„í•œ ìƒ˜í”Œ íŒŒì¼.

---

##### ë°©ë²•ë¡ 
1. ë¬¸ì œì— ëŒ€í•œ ì •ë³´ ìˆ˜ì§‘
  * ë¬¸ì œ ì •ì˜
2. Bank Churn Data setì„ ì´ìš©í•œ EDA ë° Data Cleaning
  * ê³µí†µ ì½”ë“œ
  * ë¶„ì„
    * Bank Churn Data setì— ëŒ€í•œ ê¸°ë³¸ì ì¸ ì •ë³´
    * feature ë¶„ì„
    * Data cleaning
    * Feature Engineering
3. ëª¨ë¸ í•™ìŠµ
  * XGBoost
  * LightGBM
  * CatBoost
  * Data Leakage
4. ê²°ë¡ 
  * í•œê³„ì 

---

### ë¬¸ì œì— ëŒ€í•œ ì •ë³´ ìˆ˜ì§‘
   #### 1. ë¬¸ì œ ì •ì˜
Bank Churn Datasetì€ kaggleì—ì„œ 2024ë…„ì„ ë§ì´í•´ ì œê³µí•œ ê²ƒìœ¼ë¡œ ê³ ê° ì´íƒˆ(Churn) ì˜ˆì¸¡ì„ ì§„í–‰í•˜ëŠ” ê²ƒì´ë‹¤. ì¦‰, ê³ ê°ì´ ê³„ì†í•´ì„œ ì„œë¹„ìŠ¤ë¥¼ ì´ìš©í• ì§€ ì•„ë‹ˆë©´ ê³„ì •ì„ ë‹«ê³  ì„œë¹„ìŠ¤ë¥¼ ì¤‘ë‹¨í• ì§€ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒìœ¼ë¡œ ì´ì§„ë¶„ë¥˜ ë¬¸ì œì´ë‹¤. ë‹¤ë§Œ ì´ë²ˆ kaggle ëŒ€íšŒëŠ” ì´íƒˆí•  ê°€ëŠ¥ì„±ì„ ë‚˜íƒ€ë‚´ëŠ” ì˜ˆì¸¡í•˜ëŠ” ê²ƒìœ¼ë¡œ ì œì¶œí•  ë•ŒëŠ” ì´ì§„ ë¶„ë¥˜ì˜ í™•ë¥  ê°’ì„ ì œì¶œí•´ì•¼ í•œë‹¤. ì¦‰, ì˜ˆì¸¡í•œ ê°’ì´ 0 ë˜ëŠ” 1ê³¼ ê°™ì€ ì´ì§„ ê°’ì´ ì•„ë‹Œ, 0ê³¼ 1 ì‚¬ì´ì˜ í™•ë¥ ì´ì–´ì•¼ í•˜ë¯€ë¡œ íšŒê·€ì²˜ëŸ¼ ë³´ì¼ ìˆ˜ ìˆì§€ë§Œ, ê·¼ë³¸ì ìœ¼ë¡œëŠ” ë¶„ë¥˜ ë¬¸ì œì´ë‹¤.

ì œê³µë˜ëŠ” ë°ì´í„°ëŠ” train.csv, test.csv, sample_submission.scv ì„¸ ê°œì˜ íŒŒì¼ë¡œ train.csvë¥¼ í† ëŒ€ë¡œ test.csvì˜ ê³ ê° ì´íƒˆ ì—¬ë¶€ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë¬¸ì œì´ë‹¤. ì´í›„ sample_submission.csvì— ì…ë ¥í•œ í›„ ì œì¶œí•˜ëŠ” í•´ë‹¹ íŒŒì¼ì„ ì œì¶œí•˜ëŠ” ê²ƒì´ë‹¤.

### Bank Churn Data setì„ ì´ìš©í•œ EDA ë° Data Cleaning
   #### 1. ê³µí†µ ì½”ë“œ
import libraries and files
```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import optuna
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, precision_recall_curve, roc_curve, classification_report, roc_auc_score
from lightgbm import LGBMClassifier
from sklearn.preprocessing import LabelEncoder, StandardScaler
import warnings
warnings.filterwarnings('ignore')

RANDOM_STATE = 110
pd.set_option('display.max_columns', None)

train_df = pd.read_csv("../../data/Bank_Churn_Dataset/train.csv")
test_df = pd.read_csv("../../data/Bank_Churn_Dataset/test.csv")
submission_df = pd.read_csv("../../data/Bank_Churn_Dataset/sample_submission.csv")
```
í•„ìì˜ ê²½ìš° íŒŒì¼ì„ ë”°ë¡œ ì €ì¥í•´ ë‘ëŠ” í´ë”ê°€ ìˆê¸° ë•Œë¬¸ì— ê·¸ê³³ì— ì €ì¥ì„ í•´ë‘ê³  ìˆë‹¤. ë”°ë¼ì„œ ì½”ë“œê°€ í•´ë‹¹ í´ë”ë¥¼ ì°¾ì•„ê°€ì•¼ í•˜ê¸° ë•Œë¬¸ì— ìœ„ì™€ ê°™ì´ ì‘ì„±ëœ ê²ƒì´ë‹¤.

í‰ê°€ë¥¼ ìœ„í•œ í•¨

![image](https://github.com/user-attachments/assets/6c67834d-b07c-4faa-adf9-85762112d42a)

kaggleì—ì„œ ê³µì§€í•œ í‰ê°€ ë°©ì‹ì€ ìœ„ì™€ ê°™ë‹¤. ROC ê³¡ì„  ì•„ë˜ì˜ ë©´ì (AUC)ì´ ëª¨ë¸ì˜ ìµœì¢… ì„±ëŠ¥ ì§€í‘œë¡œ ì‚¬ìš©ëœë‹¤. ì¦‰, AUC-ROCëŠ” ëª¨ë¸ì´ íƒ€ê²Ÿ í´ë˜ìŠ¤(ì´íƒˆ ë˜ëŠ” ìœ ì§€)ë¥¼ ì–¼ë§ˆë‚˜ ì˜ êµ¬ë¶„í•˜ëŠ”ì§€ í‰ê°€í•˜ëŠ” ì¤‘ìš”í•œ ì§€í‘œì´ë‹¤. ë”°ë¼ì„œ ì˜¤ì°¨í–‰ë ¬, ì •í™•ë„, ì •ë°€ë„, ì¬í˜„ìœ¨, F1 Score, AUCë¥¼ ëª¨ë‘ í™•ì¸í•  ê²ƒì´ë‹¤.

   #### 2. ë¶„ì„
   ##### 1. Bank Churn Data setì— ëŒ€í•œ ê¸°ë³¸ì ì¸ ì •ë³´
```
train_df.shape
test_df.shape
submission_df.shape

(165034, 14)
(110023, 13)
(110023, 2)
```
ìœ„ì™€ ê°™ì´ ì¶œë ¥ëœë‹¤. ì¦‰, train_dfë¡œ ëª¨ë¸ì„ í•™ìŠµì„ í•œ í›„ test_dfë¥¼ ì˜ˆì¸¡í•œ ë‹¤ìŒ submission_dfì— ì…ë ¥í•´ ì œì¶œí•˜ëŠ” ê²ƒì´ë‹¤.

train_dfëŠ” indexì™€ idë¥¼ ì œì™¸í•œ 12ê°œì˜ featureë¥¼ ê°€ì§€ê³  ìˆìœ¼ë©° ì•„ë˜ dataframeê³¼ ê°™ë‹¤.

![image](https://github.com/user-attachments/assets/7ba94325-baa0-41a4-bb4d-8eca5c37aeea)

ê°ê° featureì— ëŒ€í•œ ì„¤ëª…ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

* Surname: ì„±(ì´ë¦„)
* CreditScore: ì‹ ìš© ì ìˆ˜
* Geography: ê±°ì£¼ êµ­ê°€
* Gender: ì„±
* Age: ë‚˜ì´
* Tenure: ì€í–‰ì„ ì´ìš©í•œ ê¸°ê°„
* Balance: ê³„ì¢Œ ì”ì•¡
* NumOfProducts: ì´ìš©í•˜ëŠ” ì€í–‰ ìƒí’ˆì˜ ìˆ˜(ex. ì˜ˆê¸ˆ,ì ê¸ˆ)
* HasCrCard: ì‹ ìš©ì¹´ë“œ ë³´ìœ  ì—¬ë¶€
* IsActiveMember: í™œì„± íšŒì› ì—¬ë¶€
* EstimatedSalary: ì˜ˆìƒ ì—°ë´‰
* Exited: ì´íƒˆ ì—¬ë¶€

```
train_df.describe()
```
![image](https://github.com/user-attachments/assets/a21cdec6-6ad2-42ad-b97d-8ad6cc21f0ea)

ëŒ€ëµì ìœ¼ë¡œ ë³¼ ë•Œ ì´ìƒì¹˜ê°€ ë³´ì´ì§€ëŠ” ì•Šë‹¤. ë‹¤ë§Œ CreditScoreì™€ Ageê°€ ë„“ê²Œ ë¶„í¬í•œ ê²ƒì€ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ ë²”ìœ„ë¥¼ íŠ¹ì •í•´ì„œ êµ¬ë¶„í•˜ëŠ” ê²ƒë„ ì¢‹ì€ ë°©ë²•ì´ë¼ê³  ìƒê°í•œë‹¤.
```
target_cnt = train_df['Exited'].count()
print(train_df['Exited'].value_counts())
train_df['Exited'].value_counts() / target_cnt

Exited
0    130113
1     34921

Exited
0    0.788401
1    0.211599
```
Exitedì— ëŒ€í•œ  countë¥¼ ë³´ë©´ ë¶ˆê· í˜•ì ì¸ ë°ì´í„°ì¸ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ sampling ì‘ì—…ì´ ì¶”ê°€ì ìœ¼ë¡œ í•„ìš”í•˜ë‹¤.

   ##### 2. Data cleaning
1. data setì— ëª¨ë“  ê°’ì´ NaNì¸ ì»¬ëŸ¼ì„ í™•ì¸ ë° drop
```
all_nan_columns = train_df.columns[train_df.isna().all()].tolist()
print(f"ëª¨ë“  ê°’ì´ NaNì¸ ì»¬ëŸ¼ ê°œìˆ˜: {len(all_nan_columns)}")

train_df.drop(columns=all_nan_columns, inplace=True, axis=1)
test_df.drop(columns=all_nan_columns, inplace=True, axis=1)
```
ì´ë¯¸ ì•ì—ì„œ í™•ì¸í–ˆë“¯ ê²°ê³¼ëŠ” 0ê°œë¡œ ì—†ë‹¤.

2. ê³ ìœ ê°’ì´ 1ì¸ ì»¬ëŸ¼ í™•ì¸ ë° drop
```
unique_one_columns = [col for col in train_df.columns if train_df[col].nunique() == 1]
print(f'ê³ ìœ ê°’ì´ 1ì¸ ì»¬ëŸ¼ ê°œìˆ˜: {len(unique_one_columns)}')

train_df.drop(columns=unique_one_columns, inplace=True, axis=1)
test_df.drop(columns=unique_one_columns, inplace=True, axis=1)
```
3. ì¤‘ë³µ ë°ì´í„° drop
```
train_df.drop(['id', 'CustomerId'], axis=1, inplace=True)
test_df.drop(['id', 'CustomerId'], axis=1, inplace=True)
```
ì¤‘ë³µ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê¸° ì „ì— idì™€ CustomerIdë¥¼ ë¨¼ì € dropí•œ í›„ ì§„í–‰í–ˆë‹¤.
```
train_df.duplicated().sum()
```
54ê°œì˜ ì¤‘ë³µëœ ë°ì´í„°ê°€ ìˆë‹¤. ì¦‰, 54ê°œì˜ í–‰ì´ ë‹¤ë¥¸ í–‰ê³¼ ë™ì¼í•œ ê°’ì„ ê°€ì§€ê³  ìˆë‹¤ëŠ” ëœ»ì´ë‹¤. ì¤‘ë³µëœ ë°ì´í„°ê°€ ì¤‘ìš”í•œ ë°ì´í„°ì¸ì§€ í™•ì¸ì„ í•œ í›„ì— dropì„ í•´ì•¼ í•œë‹¤. 

ì˜ˆë¥¼ ë“¤ì–´ ì„¤ë¬¸ ì¡°ì‚¬ë‚˜ íˆ¬í‘œì™€ ê°™ì€ ë°ì´í„°ì—ì„œëŠ” ë™ì¼í•œ ì‘ë‹µì´ ì—¬ëŸ¬ ë²ˆ ìˆì„ ìˆ˜ ìˆìœ¼ë©°, ì´ëŸ° ë°ì´í„°ëŠ” ì¤‘ìš”í•œ ê°’ì´ê¸° ë•Œë¬¸ì´ë‹¤. ë°˜ë©´ ì¤‘ë³µìœ¼ë¡œ ìˆ˜ì§‘ëœ ë°ì´í„°ë¼ë©´ ì‚­ì œí•´ë„ ë˜ëŠ” ë°ì´í„°ì´ë‹¤. ë”°ë¼ì„œ í™•ì¸ì´ í•„ìš”í•˜ë‹¤.
```
duplicate_all = train_df[train_df.duplicated(keep=False)]
duplicate_all[duplicate_all['Surname']== 'Cunningham']
```
![image](https://github.com/user-attachments/assets/8dd98264-6b49-4a58-b600-b6a547c41653)

ë‹¤ìŒê³¼ ê°™ì´ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ëŒ€í‘œì ìœ¼ë¡œ Cunninghamì´ë€ ì„±ì„ ê°€ì§„ ì‚¬ëŒì˜ ë°ì´í„°ë¥¼ ì¶œë ¥í•œ ê²ƒìœ¼ë¡œ idì™€ CustomerIdëŠ” ë‹¤ë¥´ì§€ë§Œ ë‹¤ë¥¸ ê²ƒì€ ëª¨ë‘ ê°™ë‹¤. ì‹ ìš©ì ìˆ˜, ë‚˜ì´, ê±°ì£¼ êµ­ê°€, ì„±ë³„, ë‚˜ì´, ì€í–‰ ì´ìš© ê¸°ê°„, ê³„ì¢Œ ì”ì•¡ ë“± ì „ë¶€ ê°™ë‹¤. íŠ¹íˆ, í•„ìëŠ” ì„œë¡œ ë‹¤ë¥¸ ì‚¬ëŒì´ ì‹ ìš©ì ìˆ˜, ë‚˜ì´, ì€í–‰ ì´ìš© ê¸°ê°„, ì˜ˆìƒ ì—°ë´‰ ëª¨ë‘ ê°™ë‹¤ëŠ” ê²ƒì€ ë¶ˆê°€ëŠ¥ í•˜ë‹¤ ìƒê°í•˜ê¸° ë•Œë¬¸ì— ì‚­ì œí•˜ê¸°ë¡œ í–ˆë‹¤.
```
train_df = train_df.drop_duplicates()
```
4. ì¤‘ë³µëœ ë°ì´í„° ì¤‘ì—ì„œ íƒ€ê²Ÿ ê°’(Exited)ì´ ë‹¤ë¥¸ ë°ì´í„° ì°¾ê¸° - noise ì°¾ê¸°
```
y = train_df['Exited']
X = train_df.drop(['Exited'], axis=1)

train_with_target = pd.concat([X, y], axis=1)

duplicates = train_with_target.duplicated(keep=False)
duplicates_with_different_target = duplicates & (train_with_target.groupby(list(X.columns))['Exited'].transform('nunique') > 1)

noise = train_with_target[duplicates_with_different_target]
cleaned_train = train_with_target[~duplicates_with_different_target]

X = cleaned_train.drop('Exited', axis=1)
y = cleaned_train['Exited']
```
noise ë°ì´í„°ëŠ” ì—†ë‹¤. ë”°ë¼ì„œ ì—¬ê¸°ì„œ Data cleaning ì‘ì—…ì€ ë§ˆë¬´ë¦¬ í•˜ê² ë‹¤.

   ##### 2. Data cleaning
1. ìƒê´€ê´€ê³„
ìˆ«ìí˜• featureì— ëŒ€í•´ ìƒê´€ê´€ê³„ë¥¼ ë³´ê² ë‹¤. ì•„ë˜ ì½”ë“œë¥¼ ì…ë ¥í•˜ë©´ ì•„ë˜ ì‚¬ì§„ì²˜ëŸ¼ ìƒê´€ê´€ê³„ë¥¼ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ëª¨ë“  ì»¬ëŸ¼ì´ ìê¸° ìì‹ ì„ ë¹¼ê³  ì—°ê´€ì´ ê±°ì˜ ì—†ë‹¤.
```
numeric_df = train_df.select_dtypes(include=['float', 'int']).columns
numeric_df = train_df[numeric_df]

corr = numeric_df.corr()
corr.style.background_gradient(cmap='coolwarm')
```
![image](https://github.com/user-attachments/assets/2a424c73-87b9-490e-99e4-819990dbe076)

2. Age

ë‚˜ì´ëŠ” ì€í–‰ ì„œë¹„ìŠ¤ë¥¼ ê³„ì† ì´ìš©í• ì§€ ì•ˆ í• ì§€ë¥¼ ê²°ì •í•˜ëŠ” ë° ì¢‹ì€ ë°ì´í„°ë¼ê³  ìƒê°í•œë‹¤. ì´ìœ ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. ê³ ê°ì˜ ë‚˜ì´ì— ë”°ë¼ ê¸ˆìœµ ì„œë¹„ìŠ¤ì— ëŒ€í•œ í•„ìš”ì™€ ì„ í˜¸ê°€ ë‹¤ë¥´ê¸° ë•Œë¬¸ì´ë‹¤. ì¦‰ ì Šì€ ì¸µì€ ì€í–‰ì´ë‚˜ ê¸ˆìœµ ì„œë¹„ìŠ¤ë¥¼ ì ê·¹ì ìœ¼ë¡œ ë¹„êµí•˜ë©°, ë” ë‚˜ì€ í˜œíƒì´ë‚˜ ë” í¸ë¦¬í•œ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ëŠ” ë‹¤ë¥¸ ê¸ˆìœµ ê¸°ê´€ìœ¼ë¡œ ì´íƒˆí•  ê°€ëŠ¥ì„±ì´ í¬ë©°, ë‚˜ì´ê°€ ë§ì€ ê³ ê°ì€ ê¸°ì¡´ì˜ ê¸ˆìœµ ì„œë¹„ìŠ¤ì— ìµìˆ™í•´ì ¸ ìˆìœ¼ë©°, ì„œë¹„ìŠ¤ ë³€ê²½ì— ë”°ë¥¸ ë¶ˆí¸ì„ í”¼í•˜ë ¤ê³  í•˜ê¸° ë•Œë¬¸ì— ì„œë¹„ìŠ¤ë¥¼ ìœ ì§€í•˜ëŠ” ê²½í–¥ì´ ìˆì„ ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤.

ë‚˜ì´ì˜ ë¶„í¬ê°€ 18~92ê¹Œì§€ ë‹¤ì–‘í•˜ê²Œ ìˆë‹¤. ë”°ë¼ì„œ ì—°ë ¹ëŒ€ë¥¼ íŠ¹ì • êµ¬ê°„ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ì‘ì—…ì„ í•˜ê² ë‹¤.
```
def get_age(age):
    cat = ''
    if age <= 23: cat = 'Student'
    elif age <= 39: cat = 'Young Adult'
    elif age <= 64: cat = 'Adult'
    else: cat = 'Elderly'        
    return cat

group_names = ['Student', 'Young Adult', 'Adult', 'Elderly']
 
train_df['Age_range'] = train_df['Age'].apply(lambda x : get_age(x))
test_df['Age_range'] = test_df['Age'].apply(lambda x : get_age(x))
```
```
age_range_counts = train_df.groupby('Age_range')['Exited'].value_counts(normalize=True).unstack()
age_range_counts
```
![image](https://github.com/user-attachments/assets/88fc4b86-3dd4-4007-89ce-1f41b5d70f16)

ê²°ê³¼ë¥¼ ë³´ë©´ í•„ìê°€ ì˜ˆìƒí•œ ê²ƒê³¼ ë‹¤ë¥´ê²Œ Adultì—ì„œ ì´íƒˆ ê³ ê°ì´ ê°€ì¥ ë§ì•˜ê³  ë‹¤ìŒìœ¼ë¡œ Elderlyê°€ ë§ì•˜ë‹¤. Student, Young Adultê°€ ì´íƒˆ ê³ ê°ì´ ì ì—ˆë‹¤. ë”°ë¼ì„œ ë‹¤ë¥¸ ë°ì´í„°ë¥¼ í™•ì¸í•´ ë³¼ í•„ìš”ê°€ ìˆë‹¤.

3. NumOfProduct

ì€í–‰ì—ì„œ ì´ìš©í•˜ëŠ” ìƒí’ˆì˜ ìˆ˜ì´ë‹¤. ìƒí’ˆì˜ ìˆ˜ê°€ ë§ìœ¼ë©´ ë‹¤ë¥¸ ì€í–‰ì—ì„œë„ ê±°ë˜ë¥¼ í•˜ê³  ìˆì„ ê°€ëŠ¥ì„±ì´ ë†’ì•„ ì´íƒˆ ê°€ëŠ¥ì„±ì´ ë” ë†’ì„ ìˆ˜ ìˆë‹¤. ë¬¼ë¡  ë§ì€ ìƒí’ˆì„ ì´ìš©í•˜ëŠ” ê³ ê°ì€ í•´ë‹¹ ì€í–‰ê³¼ ë” ê¹Šì€ ê´€ê³„ë¥¼ í˜•ì„±í•˜ê¸° ë•Œë¬¸ì— ì´íƒˆí•  ê°€ëŠ¥ì„±ì´ ì ì„ ìˆ˜ë„ ìˆë‹¤.
```
Products_exited_counts = numeric_df.groupby('NumOfProducts')['Exited'].value_counts(normalize=True).unstack()
Products_exited_counts
```
![image](https://github.com/user-attachments/assets/5342a783-c3c4-4c43-be1f-860eab26a794)

ê²°ê³¼ëŠ” ìƒí’ˆì„ ë§ì´ ì‚¬ìš©í•  ìˆ˜ë¡ ì´íƒˆí•˜ëŠ” ì‚¬ëŒì´ ì•ë„ì ìœ¼ë¡œ ë†’ì€ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. (ë³¸ ë°ì´í„°ëŠ” ì‹¤ì œ ì€í–‰ ë°ì´í„°ê°€ ì•„ë‹ˆê¸° ë•Œë¬¸ì— ì‹¤ì œì™€ ë‹¤ë¥¼ ìˆ˜ ìˆë‹¤.) íŠ¹íˆ, 1ê°œë¥¼ ì´ìš©í•  ë•Œ ì—­ì‹œ ì´íƒˆìœ¨ì´ ë†’ë‹¤ëŠ” ê²ƒë„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.
```
train_df.groupby('Age_range')['NumOfProducts'].value_counts(normalize=True).unstack()
```
![image](https://github.com/user-attachments/assets/9a0dd1a3-09d9-4686-b7a4-325c2649029f)

ì—°ë ¹ëŒ€ê°€ ë†’ì„ ìˆ˜ë¡ ì€í–‰ì—ì„œ ì´ìš©í•˜ëŠ” ìƒí’ˆ ìˆ˜ê°€ ë§ë‹¤. ì „ì²´ì ìœ¼ë¡œ ë³´ë©´ ì „ì²´ ì—°ë ¹ëŒ€ì—ì„œ 1-2ê°œë¥¼ ì´ìš©í•˜ëŠ” ë¹„ìœ¨ì´ ë†’ì§€ë§Œ ì´íƒˆìœ¨ì´ ê°€ì¥ ë§ì€ Adult, Elderlyì—ì„œ 1ê°œë§Œ ì´ìš©í•˜ëŠ” ë¹„ìœ¨ì´ ë†’ìœ¼ë©° 3-4ê°œë¥¼ ì´ìš©í•˜ëŠ” ë¹„ìœ¨ ì—­ì‹œ ë†’ë‹¤. ì•„ë˜ í‘œëŠ” Adult, Elderlyì¼ ë•Œ Studentì™€ Young Adult ê°„ì˜ ì´ìš© ìƒí’ˆ ìˆ˜ê°€ 3~4ê°œ ì¼ ë•Œì˜ ë¹„ìœ¨ì„ ë¹„êµí•œ ê²ƒì´ë‹¤. ì´ë ‡ê²Œ ë¹„êµë¥¼ í†µí•´ Adultì™€ Elderlyê°€ ì´íƒˆ ê°€ëŠ¥ì„±ì´ ë” ë†’ì€ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

<img width="716" alt="image" src="https://github.com/user-attachments/assets/bc8cc872-2ee9-464d-8a01-63fdbfa456b4">


4. Tenure - ì€í–‰ ì´ìš© ê¸°ê°„

ì€í–‰ ì´ìš© ê¸°ê°„ ì—­ì‹œ ì€í–‰ ì„œë¹„ìŠ¤ ì´ìš©ì— ìˆì–´ í° ì˜í–¥ì„ ì¤„ ê²ƒìœ¼ë¡œ ìƒê°í•˜ê³  ìˆë‹¤. ê·¸ ì´ìœ ëŠ” ì‹ ìš© ì¹´ë“œ, ëŒ€ì¶œ, ì €ì¶• ê³„ì¢Œ ë“±ì„ ì—¬ëŸ¬ í•´ ë™ì•ˆ ì´ìš©í–ˆì„ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ì˜¤ëœ ê¸°ê°„ ì´ìš©í•  ê²½ìš° ì€í–‰ì— í° ì‹¤ë§ì„ í•˜ì§€ ì•ŠëŠ” ì´ìƒ ì€í–‰ì„ ë°”ê¾¸ì§€ ì•Šê¸° ë•Œë¬¸ì´ë‹¤.
```
tenure_exited_counts = numeric_df.groupby('Tenure')['Exited'].value_counts(normalize=True).unstack()
tenure_exited_counts
```
![image](https://github.com/user-attachments/assets/415f639a-b410-4f3d-8db4-15977144d4ba)

í•˜ì§€ë§Œ ì˜ˆìƒê³¼ ë‹¤ë¥´ê²Œ ê¸°ê°„ì´ ê¸¸ ë‹¤ê³  ì´íƒˆìœ¨ì´ ë†’ì€ ê²ƒì€ ì•„ë‹ˆì˜€ë‹¤. ë¬¼ë¡  Tenure ì¦‰, ì´ìš© ê¸°ê°„ì´ 0ì¼ ë•Œ ì´íƒˆìœ¨ì´ ê°€ì¥ ë†’ì§€ë§Œ ëŒ€ì²´ì ìœ¼ë¡œ ë¹„ìŠ·í•˜ë‹¤.

ì´ë²ˆì—ëŠ” ì´ìš© ê¸°ê°„ ë™ì•ˆ ì–¼ë§ˆë‚˜ ë§ì€ ìƒí’ˆì„ ì´ìš©í–ˆëŠ” ê°€ë¥¼ ê³„ì‚°í•´ì„œ ì»¬ëŸ¼ìœ¼ë¡œ ë§Œë“¤ì–´ ë³´ê² ë‹¤.
```
train_df['Products_Per_Tenure'] =  train_df['Tenure'] / train_df['NumOfProducts']
test_df['Products_Per_Tenure'] =  test_df['Tenure'] / test_df['NumOfProducts']
```
```
products_per_tenure_counts = train_df.groupby('Products_Per_Tenure')['Exited'].value_counts(normalize=True).unstack()

products_per_tenure_counts.plot(kind='bar', stacked=True, figsize=(12, 6))

plt.xlabel('Products per Tenure')
plt.ylabel('Proportion of Exited')
plt.legend(title='Exited', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()
```
![image](https://github.com/user-attachments/assets/ba9037b5-19ef-4848-861e-58fef06ece27)

ëª¨ë“  ê²½ìš°ì—ì„œ ì ìš©ë˜ëŠ” ê²ƒì€ ì•„ë‹ˆì§€ë§Œ ìƒí’ˆì„ ë§ì´ ì´ìš©í–ˆì„ ë•Œ ì´íƒˆìœ¨ì´ ì ê³  ì ê²Œ ì´ìš©í–ˆì„ ë•Œ ì´íƒˆìœ¨ì´ ë†’ë‹¤ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

 

5. CreditScore

CreditScore ì—­ì‹œ Ageì™€ ë¹„ìŠ·í•˜ê²Œ ë‹¤ì–‘í•˜ê²Œ ìˆê¸° ë•Œë¬¸ì— íŠ¹ì • êµ¬ê°„ìœ¼ë¡œ ë‚˜ëˆ„ê² ë‹¤. ë¯¸êµ­ ì‹ ìš© ì ìˆ˜ì¸ Fico Scoreë¥¼ ê¸°ì¤€ìœ¼ë¡œ í–ˆë‹¤. ì‹ ìš© ì ìˆ˜ê°€ ë†’ì€ ê³ ê°ì€ ì¬ì •ì ìœ¼ë¡œ ì•ˆì •ì ì´ë©°, ë” ë§ì€ ê¸ˆìœµ ìƒí’ˆ(ëŒ€ì¶œ, ì‹ ìš© ì¹´ë“œ ë“±)ì„ ì´ìš©í•  ê°€ëŠ¥ì„±ì´ ë†’ë‹¤. ë˜í•œ, ì€í–‰ê³¼ì˜ ê´€ê³„ê°€ ê¸ì •ì ì¼ ê°€ëŠ¥ì„±ì´ í¬ê¸° ë•Œë¬¸ì— ì´íƒˆ ê°€ëŠ¥ì„±ì´ ë‚®ì„ ìˆ˜ ìˆë‹¤.
```
def get_fico(age):
    cat = ''
    if age <= 579: cat = 'Poor'
    elif age <= 669: cat = 'Not_Good'
    elif age <= 799: cat = 'Very_Good'
    else: cat = 'Excellent'        
    return cat

group_names = ['Poor', 'Not_Good', 'Very_Good', 'Excellent']
 
train_df['Fico_Score'] = train_df['CreditScore'].apply(lambda x : get_fico(x))
test_df['Fico_Score'] = test_df['CreditScore'].apply(lambda x : get_fico(x))
```
```
credit_exited_counts = train_df.groupby('Fico_Score')['Exited'].value_counts(normalize=True).unstack()
credit_exited_counts
```
![image](https://github.com/user-attachments/assets/50a9b73f-4f35-4791-ae6c-e2f05eca6077)

ì˜ˆìƒê³¼ ë‹¤ë¥´ê²Œ íŠ¹ë³„í•˜ê²Œ í° ì°¨ì´ê°€ ë³´ì´ì§€ëŠ” ì•Šì§€ë§Œ Poor êµ¬ê°„ì—ì„œ ê°€ì¥ ë†’ì€ ì´íƒˆìœ¨ì„ ë³´ì—¬ì£¼ê³  ìˆë‹¤.

6. Balance(ê³„ì¢Œ ì”ì•¡), EstimatedSalary(ì˜ˆìƒ ì—°ë´‰)

ê³„ì¢Œ ì”ì•¡ê³¼ ì˜ˆìƒ ì—°ë´‰ì€ ê°™ì´ ë¹„êµë¥¼ í•˜ê² ë‹¤. ì´ìœ ëŠ” ëŒ€ë¶€ë¶„ ì—°ë´‰ì´ ë†’ì„ ìˆ˜ë¡ ê³„ì¢Œ ì”ì•¡ì´ ë§ê¸° ë•Œë¬¸ì´ë‹¤. ë‘ ë°ì´í„°ëŠ” ê³ ê°ì˜ ì¬ì •ì  ì•ˆì •ì„±ì„ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œë¡œ ì¤‘ìš”í•˜ë‹¤.
```
train_df['Balance_to_EstimatedSalary'] = train_df['Balance'] / train_df['EstimatedSalary']
test_df['Balance_to_EstimatedSalary'] = test_df['Balance'] / test_df['EstimatedSalary']
```
```
credit_exited_counts = train_df.groupby('Balance_to_EstimatedSalary')['Exited'].value_counts(normalize=True).unstack()
credit_exited_counts
```
![image](https://github.com/user-attachments/assets/51120ece-aae2-4b1f-a7d6-afc6848feb4c)

```
nan_exited_1 = credit_exited_counts[credit_exited_counts[1].isna()]
nan_exited_1
```
![image](https://github.com/user-attachments/assets/7107020c-3ff9-4c69-9f07-27e26055ff92)

ë§ì€ í–‰ì—ì„œ 1ì´ NaN ê°’ì´ë‹¤. í•˜ì§€ë§Œ  ë³´ìœ  ê¸ˆì•¡ê³¼ ì˜ˆìƒ ì—°ë´‰ì´ ì´íƒˆìœ¨ê³¼ëŠ” í¬ê²Œ ì—°ê´€ì´ ì—†ëŠ” ê²ƒ ê°™ë‹¤.

7. category feature
```
sum_columns = ['Surname', 'Geography', 'Gender']
```
ìœ„ì˜ ì„¸ ê°œì˜ featureê°€ ì•„ì§ ì‚¬ìš©í•˜ì§€ ì•Šê³  ë‚¨ì€ featureë‹¤ Surnameì˜ ê²½ìš° ì„±(ì´ë¦„)ìœ¼ë¡œ êµ­ê°€, ì„±ë³„ê³¼ ê²°í•©í•˜ë©´ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ì¤„ ìˆ˜ ìˆë‹¤ê³  ì˜ˆìƒí•˜ê³  ìˆì–´ì„œ ê²°í•©í•˜ê¸°ë¡œ í–ˆë‹¤.
```
train_df['Surename_Geography_Gender'] = train_df[sum_columns].apply(lambda x: '_'.join(x.astype(str)), axis=1)
test_df['Surename_Geography_Gender'] = test_df[sum_columns].apply(lambda x: '_'.join(x.astype(str)), axis=1)
train_df
```
ê²°ê³¼ì ìœ¼ë¡œ ì•„ë˜ì™€ ê°™ì´ featureê°€ ë§Œë“¤ì–´ ì¡Œë‹¤.
![image](https://github.com/user-attachments/assets/91b92fa3-bb9a-455e-a22e-c082c48e65e2)

### ëª¨ë¸ í•™ìŠµ
XGBoostì™€ LightGBMì€ CatBoostì™€ ë‹¤ë¥´ê²Œ ë¼ë²¨ë§ ì‘ì—…ì„ í•´ì•¼í•œë‹¤. ë”°ë¼ì„œ XGBoostì™€ LightGBMì— ì ìš©í•  ê³µí†µ ì½”ë“œë¥¼ ë¨¼ì € ì‘ì„±í•˜ê² ë‹¤.
```
y = train_df['Exited']
X = train_df.drop(['Exited','Surname'], axis=1)
test_df = test_df.drop(['Surname'], axis=1)
```
ë¨¼ì € Surnameì„ dropí•˜ê² ë‹¤. Surnameì€ train ë°ì´í„°ì— ì—†ëŠ” ê²ƒì´ test ë°ì´í„°ì—ëŠ” í¬í•¨ë˜ì–´ ìˆì„ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— Lable Encodingì„ í•  ë•Œ LabelEncoder.classes_ ì„ ì´ìš©í•˜ëŠ” ë°©ë²•ë„ ìˆì§€ë§Œ ì²˜ìŒ ë³´ëŠ” ë°ì´í„°ëŠ” ì˜ëª» í•™ìŠµí•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ì œê±°í•˜ê¸°ë¡œ í–ˆë‹¤. 
```
X.drop(['Surename_Geography_Gender'], axis=1, inplace=True)
test_df.drop(['Surename_Geography_Gender'], axis=1, inplace=True)
```
ìœ„ ì½”ë“œ ì—­ì‹œ  Surnameì´ í¬í•¨ëœ featureë¥¼ ì œê±°í•˜ëŠ” ê²ƒìœ¼ë¡œ ì´ìœ ëŠ” ìœ„ì™€ ê°™ë‹¤.
```
numeric_X = X.select_dtypes(include=['float', 'int']).columns
category_X = X.select_dtypes(include=['object']).columns
```
ìŠ¤ì¼€ì¼ë§ê³¼ ì¸ì½”ë”©ì„ ìœ„í•´ numeric featureì™€ ì•„ë‹Œ featureë¥¼ êµ¬ë¶„í–ˆë‹¤. ì´í›„ ë°‘ì— ìˆëŠ” ì½”ë“œì™€ ê°™ì´ ìŠ¤ì¼€ì¼ë§ê³¼ ì¸ì½”ë”©ì„ ì§„í–‰í–ˆë‹¤.
```
# scaling
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X[numeric_X] = scaler.fit_transform(X[numeric_X])
test_df[numeric_X] = scaler.transform(test_df[numeric_X])

X = pd.DataFrame(X, columns=columns)
test_df = pd.DataFrame(test_df, columns=columns)

# label encoding
from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
for i in category_X:
    encoder = LabelEncoder()
    encoder.fit(X[i])
    
    X[i] = encoder.transform(X[i])
    test_df[i] = encoder.transform(test_df[i])
```
   #### 1. XGBoost
```
num_folds=5
n_est=3500
```
```
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=RANDOM_STATE, stratify=y)
```
```
from hyperopt import hp, fmin, tpe, Trials
from sklearn.model_selection import KFold
from sklearn.metrics import roc_auc_score
from xgboost import XGBClassifier, plot_importance

xgb_search_space = {'max_depth': hp.quniform('max_depth', 2, 15, 1), 
                    'min_child_weight': hp.quniform('min_child_weight', 1, 6, 1),
                    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 0.95),
                    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2)}

def objective_func(search_space):
    xgb_clf = XGBClassifier(n_estimators=100,
                            max_depth=int(search_space['max_depth']),
                            min_child_weight=int(search_space['min_child_weight']),
                            colsample_bytree=search_space['colsample_bytree'],
                            learning_rate=search_space['learning_rate'],
                            early_stopping_rounds=30,
                            eval_metric='logloss',
                           random_state=RANDOM_STATE)
    
    roc_auc_list= []
    kf = KFold(n_splits=5)
    
    for tr_index, val_index in kf.split(X_train):
        X_tr, y_tr = X_train.iloc[tr_index], y_train.iloc[tr_index]
        X_val, y_val = X_train.iloc[val_index], y_train.iloc[val_index]
        
        xgb_clf.fit(X_tr, y_tr, eval_set=[(X_tr, y_tr), (X_val, y_val)], verbose=False)
        score = roc_auc_score(y_val, xgb_clf.predict_proba(X_val)[:, 1])
        roc_auc_list.append(score)
    return -1 * np.mean(roc_auc_list)

trials = Trials()
best = fmin(fn=objective_func,
            space=xgb_search_space,
            algo=tpe.suggest,
            max_evals=50, # ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ë¥¼ ì§€ì •í•©ë‹ˆë‹¤.
            trials=trials,
            rstate=np.random.default_rng()
           )
print('best:', best)

xgb_clf = XGBClassifier(n_estimators=500, learning_rate=round(best['learning_rate'], 5),
                        max_depth=int(best['max_depth']), min_child_weight=int(best['min_child_weight']), eval_metric="logloss",
                        colsample_bytree=round(best['colsample_bytree'], 5), random_state=RANDOM_STATE, verbose=False)

xgb_clf.fit(X_tr, y_tr, eval_set=[(X_tr, y_tr), (X_val, y_val)])
xgb_roc_score = roc_auc_score(y_test, xgb_clf.predict_proba(X_test)[:,1])
print('ROC AUC: {0:.4f}'.format(xgb_roc_score))
```
```
pred = xgb_clf.predict(X_train) 
proba = xgb_clf.predict_proba(X_train)[:, 1]

best_rf_pred = xgb_clf.predict(X_test) 
best_rf_proba = xgb_clf.predict_proba(X_test)[:, 1]

get_clf_eval(y_train, pred, proba)
get_clf_eval(y_test , best_rf_pred, best_rf_proba)
```
optunaì™€ K-Foldë¥¼ ì´ìš©í•´ ë² ìŠ¤íŠ¸ íŒŒë¼ë¯¸í„°ë¥¼ êµ¬í•œ ë‹¤ìŒ ê²°ê³¼ ê°’ì„ í™•ì¸í•´ ë³´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.
```
[[87041  4002]
 [10049 14394]]
ì •í™•ë„: 0.8783, ì •ë°€ë„: 0.7825, ì¬í˜„ìœ¨: 0.5889,    F1: 0.6720, AUC:0.9102
ì˜¤ì°¨ í–‰ë ¬
[[36944  2074]
 [ 4716  5760]]
ì •í™•ë„: 0.8628, ì •ë°€ë„: 0.7353, ì¬í˜„ìœ¨: 0.5498,    F1: 0.6292, AUC:0.8869
```
í‰ê°€ ì§€í‘œì¸ AUCëŠ” train ë°ì´í„°ì— ëŒ€í•œ AUC ê°’ì´ ë„ˆë¬´ ë†’ë‹¤ë©´(ì˜ˆ: 0.99 ì´ìƒ), ëª¨ë¸ì´ ê³¼ì í•©ë  ê°€ëŠ¥ì„±ì´ í¬ê¸° ë•Œë¬¸ì— test ë°ì´í„°ì— ëŒ€í•œ AUC ê°’ì´ ì¤‘ìš”ë‹¤. ì¦‰, test ë°ì´í„°ì—ì„œ ê°’ì´ ë†’ì„ìˆ˜ë¡, ëª¨ë¸ì´ ì¼ë°˜í™”ëœ ì„±ëŠ¥ì„ ê°€ì§€ê³  ìˆë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤. ê²°ê³¼ë¥¼ ë³´ë©´ í‰ê°€ ê¸°ì¤€ì¸ AUCê°€ ì¢‹ê²Œ ë‚˜ì™”ìœ¼ë©° trainê³¼ test ì„¸íŠ¸ì—ì„œ í° ì°¨ì´ê°€ ë‚˜ì§€ ì•Šì–´ ê³¼ì í•©ì€ ì•„ë‹ˆë‹¤. ë”°ë¼ì„œ ì¢‹ì€ ëª¨ë¸ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ ì¬í˜„ìœ¨ì´ ë‘˜ ë‹¤ ë‚®ì€ í¸ì´ë¯€ë¡œ, ì‹¤ì œ ì´íƒˆ(Exited = 1)ì„ ë†“ì¹˜ëŠ” ê²½ìš°ê°€ ë§ì„ ìˆ˜ ìˆë‹¤.
```
test_preds = np.empty((num_folds, len(test_df)))
auc_vals = []

folds = StratifiedKFold(n_splits=num_folds, random_state=RANDOM_STATE, shuffle=True)

for n_fold, (train_idx, valid_idx) in enumerate(folds.split(X, y)):
    
    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]
    X_val, y_val = X.iloc[valid_idx], y.iloc[valid_idx]
    
    xgb_clf.fit(X_train, y_train, eval_set=[(X_val, y_val)])
    
    y_pred_val = xgb_clf.predict_proba(X_val)[:, 1]
    auc_val = roc_auc_score(y_val, y_pred_val)
    print(f"AUC for fold {n_fold}: {auc_val}")
    auc_vals.append(auc_val)
    
    y_pred_test = xgb_clf.predict_proba(test_df)[:, 1]
    test_preds[n_fold, :] = y_pred_test
    print("----------------")

y_pred = test_preds.mean(axis=0)

print(f"ìµœì¢… ì˜ˆì¸¡ê°’ (y_pred): {y_pred}")
```
ë‹¤ìŒìœ¼ë¡œ k-foldë¥¼ ì´ìš©í•´ ë°ì´í„°ë¥¼ 5ê°œë¡œ ë‚˜ëˆˆ ë‹¤ìŒ ìœ„ì—ì„œ í•™ìŠµí•œ ë² ìŠ¤íŠ¸ íŒŒë¼ë¯¸í„°ë¥¼ í†µí•´ ê° ì¼€ì´ìŠ¤ë§ˆë‹¤ test ë°ì´í„°ë¥¼ ì˜ˆì¸¡í•œ í›„ test_predsì— ì €ì¥í–ˆë‹¤. ì˜ˆì¸¡í•œ í™•ë¥ ë“¤ì„ í‰ê· ì„ ë‚¸ ë‹¤ìŒ ì•„ë˜ì™€ ê°™ì´ ì œì¶œ íŒŒì¼ì˜ Exitedì— ì…ë ¥í•œ í›„ ì œì¶œí–ˆë‹¤.
```
submission_df['Exited'] = y_pred
submission_df.head()
submission_df.to_csv("submission.csv",index=False)
```
kaggleì— ì œì¶œí•˜ë©´ ë‹¤ìŒê³¼ ê°™ì´ ê²°ê³¼ê°€ ë‚˜ì˜¨ë‹¤.
![image](https://github.com/user-attachments/assets/a7c4b50f-6dbb-4978-9b71-de9f7545cdc3)

   #### 2. LightGBM
```
num_folds=5
n_est=3500
```
```
import optuna

def objective(trial):
    param = {
        'objective': 'binary',
        'metric': 'binary_logloss',
        'boosting_type': 'gbdt',
        'num_leaves': trial.suggest_int('num_leaves', 20, 60),
        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),
        'min_child_weight': trial.suggest_float('min_child_weight', 0.1, 10.0),
        'max_depth': trial.suggest_int('max_depth', 3, 15),
        'subsample': trial.suggest_float('subsample', 0.4, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.4, 1.0),
        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1, log=True),
        'scale_pos_weight': trial.suggest_float('scale_pos_weight', 1, 50),
        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 10.0),
        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 10.0),
        'n_estimators': trial.suggest_int('n_estimators', 100, 1000)
    }

    lgb_model = LGBMClassifier(**param, random_state=RANDOM_STATE, verbose=-1)
    lgb_model.fit(X_train, y_train, feature_name=['f' + str(i) for i in range(X_train.shape[1])])
    y_val_pred = lgb_model.predict(X_test)
    f1 = f1_score(y_test, y_val_pred, pos_label=1) 
    return f1

study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=200)

best_params = study.best_params
print("Best params: ", best_params)

best_lgb_model = LGBMClassifier(**best_params, random_state=RANDOM_STATE)
best_lgb_model.fit(X_train, y_train, feature_name=['f' + str(i) for i in range(X_train.shape[1])])
```
```
pred = best_lgb_model.predict(X_train) 
proba = best_lgb_model.predict_proba(X_train)[:, 1]

best_rf_pred = best_lgb_model.predict(X_test) 
best_rf_proba = best_lgb_model.predict_proba(X_test)[:, 1]

get_clf_eval(y_train, pred, proba)
get_clf_eval(y_test , best_rf_pred, best_rf_proba)
```
XGBoostì™€ ê°™ì´ optunaì™€ K-Foldë¥¼ ì´ìš©í•´ ë² ìŠ¤íŠ¸ íŒŒë¼ë¯¸í„°ë¥¼ êµ¬í•œ ë‹¤ìŒ ê²°ê³¼ ê°’ì„ í™•ì¸í•´ ë³´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.
```
ì˜¤ì°¨ í–‰ë ¬
[[93819 10230]
 [ 5401 22534]]
ì •í™•ë„: 0.8816, ì •ë°€ë„: 0.6878, ì¬í˜„ìœ¨: 0.8067,    F1: 0.7425, AUC:0.9352
ì˜¤ì°¨ í–‰ë ¬
[[34992  4026]
 [ 2258  8218]]
ì •í™•ë„: 0.8730, ì •ë°€ë„: 0.6712, ì¬í˜„ìœ¨: 0.7845,    F1: 0.7234, AUC:0.9259
```
ê³¼ì í•©ë˜ì§€ ì•Šê³  ì¢‹ì€ ì¼ë°˜í™” ì„±ëŠ¥ì„ ë³´ì´ê³  ìˆë‹¤. XGBoostë³´ë‹¤ ì¬í˜„ìœ¨ì´ ë†’ì•„ì¡Œì§€ë§Œ ì •ë°€ë„ê°€ ë‚®ì•„ì¡Œë‹¤.
```
test_preds = np.empty((num_folds, len(test_df)))
auc_vals = []

folds = StratifiedKFold(n_splits=num_folds, random_state=RANDOM_STATE, shuffle=True)

for n_fold, (train_idx, valid_idx) in enumerate(folds.split(X, y)):
    
    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]
    X_val, y_val = X.iloc[valid_idx], y.iloc[valid_idx]
    
    # ìµœì í™”ëœ íŒŒë¼ë¯¸í„°ë¡œ LightGBM ëª¨ë¸ í•™ìŠµ
    best_lgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)])
    
    # ê²€ì¦ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡
    y_pred_val = best_lgb_model.predict_proba(X_val)[:, 1]
    auc_val = roc_auc_score(y_val, y_pred_val)
    print(f"AUC for fold {n_fold}: {auc_val}")
    auc_vals.append(auc_val)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ì„ ì €ì¥
    y_pred_test = best_lgb_model.predict_proba(test_df)[:, 1]
    test_preds[n_fold, :] = y_pred_test
    print("----------------")

# ëª¨ë“  foldì—ì„œì˜ í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ê°’ í‰ê·  ê³„ì‚°
y_pred = test_preds.mean(axis=0)

print(f"ìµœì¢… ì˜ˆì¸¡ê°’ (y_pred): {y_pred}")
```
```
submission_df['Exited'] = y_pred
submission_df.head()
submission_df.to_csv("submission.csv",index=False)
```
![image](https://github.com/user-attachments/assets/adad2008-68ca-4bc3-96a5-4f88cc0a1668)

ê²°ê³¼ëŠ” XGBoostë³´ë‹¤ ì‚´ì§ ë†’ì€ ì ìˆ˜ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.

   #### 3. CatBoost
```
y = train_df['Exited']
X = train_df.drop(['Exited'], axis=1)
```
CatBoostëŠ” XGBoostì™€ LightGBMê³¼ ê°™ì´ ì¸ì½”ë”©ì„ ë”°ë¡œ ì•ˆ í•´ì¤˜ë„ ëœë‹¤. ë”°ë¼ì„œ ìœ„ì™€ ê°™ì´ targetë§Œ X, yë¡œ ë¶„ë¦¬í–ˆë‹¤.
```
numeric_X = X.select_dtypes(include=['float', 'int']).columns
```
```
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X[numeric_X] = scaler.fit_transform(X[numeric_X])
test_df[numeric_X] = scaler.transform(test_df[numeric_X])

X = pd.DataFrame(X, columns=columns)
test_df = pd.DataFrame(test_df, columns=columns)
```
ìŠ¤ì¼€ì¼ë§ì„ ìœ„í•´ numeric featureë§Œ ë”°ë¡œ ì €ì¥í•œ í›„ ìŠ¤ì¼€ì¼ë§ì„ ì§„í–‰í–ˆë‹¤.
```
cat_features = np.where(X.dtypes != np.float64)[0]
```
CatBoostì—ì„œ floatí˜•ì´ ì•„ë‹Œ ì»¬ëŸ¼ì„ ì•Œë ¤ì¤˜ì•¼ ì¸ì½”ë”©ì„ ë”°ë¡œ ì§„í–‰í•˜ì§€ ì•Šì•„ë„ ì²˜ë¦¬ê°€ ê°€ëŠ¥í•˜ë‹¤. ë”°ë¼ì„œ ë”°ë¡œ ì»¬ëŸ¼ì˜ indexë¥¼ ì¶”ì¶œí–ˆë‹¤. CatBoostê°€ ì¢‹ì€ ì ì€ test ë°ì´í„°ì—ë§Œ ì¡´ì¬í•˜ëŠ” ë°ì´í„° ì¦‰, ìƒˆë¡œìš´ ì¹´í…Œê³ ë¦¬ ê°’ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ê³ ìœ í•œ ë°©ì‹ì´ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤.

```
num_folds=5
n_est=3500
```
```
folds = StratifiedKFold(n_splits=num_folds, random_state=RANDOM_STATE, shuffle=True)
test_preds = np.empty((num_folds, len(test_df)))
auc_vals=[]

for n_fold, (train_idx, valid_idx) in enumerate(folds.split(X, y)):
    
    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]
    X_val, y_val = X.iloc[valid_idx], y.iloc[valid_idx]
    
    train_pool = Pool(X_train, y_train, cat_features=cat_features)
    val_pool = Pool(X_val, y_val, cat_features=cat_features)
    
    clf = CatBoostClassifier(eval_metric='AUC', learning_rate=0.03, iterations=n_est)
    clf.fit(train_pool, eval_set=val_pool, verbose=300)
    
    y_pred_val = clf.predict_proba(X_val[columns])[:,1]
    auc_val = roc_auc_score(y_val, y_pred_val)
    print("AUC for fold ", n_fold, ": ", auc_val)
    auc_vals.append(auc_val)
    
    y_pred_test = clf.predict_proba(test_df[columns])[:,1]
    test_preds[n_fold, :] = y_pred_test
    print("----------------")
```
k-foldë¥¼ ì´ìš©í•´ ë°ì´í„°ë¥¼ 5ê°œë¡œ ë‚˜ëˆˆ ë‹¤ìŒ ê° ì¼€ì´ìŠ¤ë§ˆë‹¤ test ë°ì´í„°ë¥¼ ì˜ˆì¸¡í•œ í›„ test_predsì— ì €ì¥í–ˆë‹¤. ì‰½ê²Œ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. ë˜í•œ, CatBoostëŠ” ìë™ìœ¼ë¡œ íŒŒë¼ë¯¸í„°ë¥¼ íŠœë‹í•´ì£¼ê¸° ë•Œë¬¸ì— optunaì™€ F-Foldë¥¼ ì´ìš©í•´ íŠœë‹ì„ í•˜ì§€ ì•Šì•˜ë‹¤.

*  Fold 1: aë¡œ ê²€ì¦, b + c + d + eë¡œ í›ˆë ¨ â†’ test_df ì˜ˆì¸¡
*  Fold 2: bë¡œ ê²€ì¦, a + c + d + eë¡œ í›ˆë ¨ â†’ test_df ì˜ˆì¸¡
*  Fold 3: cë¡œ ê²€ì¦, a + b + d + eë¡œ í›ˆë ¨ â†’ test_df ì˜ˆì¸¡
*  Fold 4: dë¡œ ê²€ì¦, a + b + c + eë¡œ í›ˆë ¨ â†’ test_df ì˜ˆì¸¡
*  Fold 5: eë¡œ ê²€ì¦, a + b + c + dë¡œ í›ˆë ¨ â†’ test_df ì˜ˆì¸¡
```
submission_df['Exited'] = y_pred
submission_df.head()
submission_df.to_csv("submission.csv",index=False)
```
ì´í›„ ìœ„ì—ì„œ í–ˆë˜ ë°©ë²•ê³¼ ê°™ì´ í‰ê· ì„ ë‚¸ y_predë¥¼ ì œì¶œ íŒŒì¼ì— ì…ë ¥í•œ í›„ ì œì¶œí–ˆë‹¤. ê²°ê³¼ëŠ” ì•„ë˜ì™€ ê°™ë‹¤. ê²°ê³¼ì ìœ¼ë¡œ CatBosotê°€ ê°€ì¥ ë†’ì€ ì ìˆ˜ë¥¼ ì–»ì„ ìˆ˜ ìˆì—ˆë‹¤.
![image](https://github.com/user-attachments/assets/a4d74ce1-e6a5-4fc7-b409-b4732d90e911)

   #### 4. Data Leakage

ì´ë²ˆ ëŒ€íšŒì—ì„œëŠ” Data Leakageê°€ ìˆì—ˆë‹¤. ë¬¼ë¡  í•„ìëŠ” Data Leakageê°€ ì¼ì–´ë‚˜ì§€ ì•Šì•˜ì§€ë§Œ ê³µìœ ëœ ì½”ë“œë¥¼ ë³´ë©´ ë†’ì€ ì ìˆ˜ë¥¼ ì–»ì€ ì‚¬ëŒë“¤ì˜ ëŒ€ë¶€ë¶„ì€ Data Leakageê°€ ìˆì—ˆë‹¤.

kaggleì—ì„œ ì œê³µí•œ íŒŒì¼ì€ train, test, submission ì´ë ‡ê²Œ ì„¸ ê°œì˜ íŒŒì¼ì´ë‹¤. í•˜ì§€ë§Œ ê³µìœ ëœ ì½”ë“œë¥¼ ë³´ë©´ Original Dataê°€ ë“±ì¥í•œë‹¤.

![image](https://github.com/user-attachments/assets/78ad183d-3e2d-4d1c-b364-78e23fd07886)

Original Dataë¥¼ í•™ìŠµ ë°ì´í„°ì— í¬í•¨í•´ì„œ ì•„ë˜ì™€ ê°™ì´ í•™ìŠµì„ í–ˆë‹¤. 
```
df_train = pd.concat([df_train, original_data], axis=0)
```
![image](https://github.com/user-attachments/assets/bb5c87a8-724e-4048-965c-06656d563eff)

í•˜ì§€ë§Œ ìœ„ì˜ ì‚¬ì§„ê³¼ ê°™ì´ ì ìˆ˜ëŠ” í•„ìì™€ ë¹„ìŠ·í•˜ê²Œ ë‚˜ì™”ë‹¤.

---

### ê²°ë¡ 
ì´ë²ˆ Kaggle ë¶„ë¥˜ ëŒ€íšŒë¥¼ í†µí•´ ê³ ê° ì´íƒˆ(Churn)ì„ ì˜ˆì¸¡í•˜ëŠ” ê³¼ì •ì„ ë‹¤ë£¨ë©´ì„œ ë‹¤ì–‘í•œ ëª¨ë¸ê³¼ ê¸°ë²•ì„ ì‹¤í—˜í–ˆìŠµë‹ˆë‹¤. ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” ê³¼ì •ì—ì„œ EDA(íƒìƒ‰ì  ë°ì´í„° ë¶„ì„)ë¥¼ í†µí•´ ì£¼ìš”í•œ íŒ¨í„´ê³¼ íŠ¹ì„±ì„ ë°œê²¬í•˜ì˜€ê³ , ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ Feature Engineeringì„ í†µí•´ ì˜ˆì¸¡ ì„±ëŠ¥ì„ ë†’ì´ê¸° ìœ„í•œ ìƒˆë¡œìš´ ë³€ìˆ˜ë¥¼ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤. ë˜í•œ, LightGBM, XGBoost, CatBoostì™€ ê°™ì€ ë‹¤ì–‘í•œ ë¶€ìŠ¤íŒ… ëª¨ë¸ì„ ì‚¬ìš©í•´ ì„±ëŠ¥ì„ í‰ê°€í–ˆìœ¼ë©°, ê° ëª¨ë¸ì˜ ì¥ë‹¨ì ê³¼ ê²°ê³¼ë¥¼ ë¹„êµí•˜ëŠ” ê³¼ì •ì„ ê±°ì³¤ìŠµë‹ˆë‹¤.

* Feature Engineeringì˜ ì¤‘ìš”ì„±
ê³ ê° ì´íƒˆ ì˜ˆì¸¡ì—ì„œ ì¤‘ìš”í•œ ë³€ìˆ˜ë¡œëŠ” ë‚˜ì´, ì‹ ìš© ì ìˆ˜, ì€í–‰ ì„œë¹„ìŠ¤ ì´ìš© ê¸°ê°„, ì‚¬ìš© ì¤‘ì¸ ê¸ˆìœµ ìƒí’ˆì˜ ìˆ˜ ë“±ì´ ìˆì—ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë³€ìˆ˜ë“¤ì€ ê³ ê°ì˜ í–‰ë™ íŒ¨í„´ì„ ë°˜ì˜í•  ìˆ˜ ìˆìœ¼ë©°, ì´íƒˆ ê°€ëŠ¥ì„±ì„ ì˜ ì˜ˆì¸¡í•  ìˆ˜ ìˆëŠ” ë³€ìˆ˜ë“¤ë¡œ ë“œëŸ¬ë‚¬ìŠµë‹ˆë‹¤. íŠ¹íˆ, ë‚˜ì´ë¥¼ ì—°ë ¹ëŒ€ ê·¸ë£¹ìœ¼ë¡œ ë‚˜ëˆ„ê³ , ê³ ê°ì´ ì´ìš©í•˜ëŠ” ìƒí’ˆ ìˆ˜ì™€ ì´ìš© ê¸°ê°„ì„ ì¡°í•©í•˜ì—¬ ë§Œë“  ë³€ìˆ˜ëŠ” ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë° ì¤‘ìš”í•œ ì—­í• ì„ í–ˆìŠµë‹ˆë‹¤.
* ëª¨ë¸ ë¹„êµ ë° ì„±ëŠ¥ í‰ê°€
ì„¸ ê°€ì§€ ëŒ€í‘œì ì¸ ë¶€ìŠ¤íŒ… ëª¨ë¸(XGBoost, LightGBM, CatBoost)ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµí•˜ê³  ê²€ì¦í•œ ê²°ê³¼, CatBoost ëª¨ë¸ì´ ê°€ì¥ ë†’ì€ AUC(Area Under the ROC Curve)ë¥¼ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤. CatBoostëŠ” ë²”ì£¼í˜• ë³€ìˆ˜ë¥¼ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•˜ê³ , í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ìƒˆë¡œìš´ ë²”ì£¼í˜• ë³€ìˆ˜ë¥¼ ì¸ì‹í•  ìˆ˜ ìˆì–´, ì´ ê³¼ì •ì—ì„œ íŠ¹ë³„í•œ ì¸ì½”ë”© ì‘ì—…ì„ ìƒëµí•  ìˆ˜ ìˆëŠ” ê°•ì ì´ ìˆì—ˆìŠµë‹ˆë‹¤. ì´ë¡œ ì¸í•´ ë” ë†’ì€ ì¼ë°˜í™” ì„±ëŠ¥ì„ ë°œíœ˜í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. LightGBMê³¼ XGBoostë„ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì˜€ìœ¼ë‚˜, ë°ì´í„°ì…‹ì˜ íŠ¹ì„±ìƒ CatBoostê°€ ë” ì í•©í•œ ëª¨ë¸ì„ì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.
* êµì°¨ ê²€ì¦ì„ í†µí•œ ëª¨ë¸ì˜ ì¼ë°˜í™”
êµì°¨ ê²€ì¦(K-Fold)ì„ í†µí•´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•œ ê²°ê³¼, ê³¼ì í•© ì—†ì´ ì•ˆì •ì ì¸ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ëŠ” ê²ƒì´ ì¤‘ìš”í•¨ì„ ë‹¤ì‹œ í•œ ë²ˆ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ê° Foldì—ì„œ ë¹„ìŠ·í•œ AUC ì ìˆ˜ë¥¼ ê¸°ë¡í•œ ëª¨ë¸ì€ ì¼ë°˜í™”ëœ ì„±ëŠ¥ì„ ê°€ì§€ê³  ìˆë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•˜ë©°, ì´ê²ƒì´ Kaggle ëŒ€íšŒì—ì„œ ì¤‘ìš”í•œ ìš”ì†Œ ì¤‘ í•˜ë‚˜ì„ì„ ëŠê¼ˆìŠµë‹ˆë‹¤. ë‹¤ì–‘í•œ Foldì—ì„œ í›ˆë ¨ê³¼ ê²€ì¦ì„ ê±°ì³ ìµœì¢… ì˜ˆì¸¡ê°’ì„ í‰ê· í™”í•˜ëŠ” ë°©ì‹ì€, ê°œë³„ Foldì—ì„œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ë¶ˆê· í˜•í•œ ë°ì´í„°ë¥¼ ë³´ì™„í•´ ì¤ë‹ˆë‹¤.
* Data Leakageì˜ ì¤‘ìš”ì„±
ì´ë²ˆ ëŒ€íšŒì—ì„œëŠ” Data Leakageê°€ ì¼ë¶€ ì°¸ê°€ìë“¤ ì‚¬ì´ì—ì„œ ë°œìƒí•œ ì ì´ ëˆˆì— ë„ì—ˆìŠµë‹ˆë‹¤. ë³¸ë˜ Kaggle ëŒ€íšŒì—ì„œëŠ” ì œê³µëœ í•™ìŠµ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë§Œì„ ì‚¬ìš©í•´ì•¼ í•˜ì§€ë§Œ, ì¼ë¶€ ì°¸ê°€ìë“¤ì€ ê³µìœ ëœ 'Original Data'ë¥¼ í•™ìŠµì— í¬í•¨í•˜ì—¬ ì ìˆ˜ë¥¼ ë†’ì˜€ìŠµë‹ˆë‹¤. Data LeakageëŠ” ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë¶€ì •í™•í•˜ê²Œ ë†’ì¼ ìˆ˜ ìˆìœ¼ë©°, ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ì‹ ë¢°í•  ìˆ˜ ì—†ëŠ” ê²°ê³¼ë¥¼ ì´ˆë˜í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ì£¼ì˜í•´ì•¼ í•  ìš”ì†Œì…ë‹ˆë‹¤. í•„ìëŠ” ì´ëŸ¬í•œ ì ì„ ì¸ì§€í•˜ê³ , ë°ì´í„° ëˆ„ì¶œì´ ì—†ëŠ” ë°©ì‹ìœ¼ë¡œ ëª¨ë¸ì„ êµ¬ì¶•í•˜ì—¬, ê³µì •í•˜ê²Œ ì„±ëŠ¥ì„ í‰ê°€í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.

#### í•œê³„ì 
* ë¶ˆê· í˜• ë°ì´í„°
ê³ ê° ì´íƒˆ ì˜ˆì¸¡ì—ì„œ íƒ€ê²Ÿ í´ë˜ìŠ¤(Exited = 1)ì™€ ë¹„ì´íƒˆ í´ë˜ìŠ¤(Exited = 0)ì˜ ë¶ˆê· í˜•ì€ ì£¼ìš”í•œ ë¬¸ì œì˜€ìŠµë‹ˆë‹¤. ì´ë¡œ ì¸í•´ ëª¨ë¸ì´ ì´íƒˆí•˜ì§€ ì•ŠëŠ” ê³ ê°ì„ ê³¼ëŒ€ í‰ê°€í•  ê°€ëŠ¥ì„±ì´ ìˆì—ˆê³ , ì´ëŠ” F1 ì ìˆ˜ì—ì„œ ë‚®ì€ ì¬í˜„ìœ¨ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì–¸ë”ìƒ˜í”Œë§, ì˜¤ë²„ìƒ˜í”Œë§, SMOTEì™€ ê°™ì€ ê¸°ë²•ì„ ì‚¬ìš©í•´ ë°ì´í„° ë¶ˆê· í˜•ì„ í•´ê²°í•  ìˆ˜ë„ ìˆì—ˆìœ¼ë‚˜, ì´ë²ˆ ëŒ€íšŒì—ì„œëŠ” ì´ëŸ¬í•œ ê¸°ë²•ì„ í™œìš©í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í–¥í›„ì—ëŠ” ë” ë³µì¡í•œ ê¸°ë²•ì„ ì ìš©í•˜ì—¬ ë¶ˆê· í˜• ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤.
* ë°ì´í„° ì´í•´ ë¶€ì¡±
ì œê³µëœ ë°ì´í„°ëŠ” ì‹¤ì œ ê¸ˆìœµ ë°ì´í„°ê°€ ì•„ë‹ˆì—ˆê¸° ë•Œë¬¸ì— ì‹¤ì œ ì€í–‰ì—ì„œ ê³ ê° ì´íƒˆì„ ì˜ˆì¸¡í•˜ëŠ” ë¬¸ì œì™€ëŠ” ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¡œ ì¸í•´, ì¼ë¶€ ë³€ìˆ˜ì˜ íŒ¨í„´ì´ ì‹¤ì œ ê¸ˆìœµ ë°ì´í„°ì—ì„œëŠ” ë‹¤ë¥´ê²Œ ë‚˜íƒ€ë‚  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, ì´ë²ˆ í”„ë¡œì íŠ¸ì—ì„œëŠ” í”¼ì²˜ì— ëŒ€í•œ ë” ê¹Šì€ ë„ë©”ì¸ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ í•œ ë³€í˜•ì´ ì œí•œì ì´ì—ˆìœ¼ë¯€ë¡œ, ì‹¤ì œ ì ìš©ì—ì„œëŠ” ì¶”ê°€ì ì¸ ë³€í˜• ë° ë„ë©”ì¸ ì§€ì‹ì´ í•„ìš”í•  ê²ƒì…ë‹ˆë‹¤.

#### ë§ˆë¬´ë¦¬
ì´ë²ˆ í”„ë¡œì íŠ¸ë¥¼ í†µí•´ ë¶„ë¥˜ ë¬¸ì œì—ì„œ ë°ì´í„° ì „ì²˜ë¦¬, Feature Engineering, ëª¨ë¸ í•™ìŠµ, ê·¸ë¦¬ê³  ì„±ëŠ¥ í‰ê°€ì— ëŒ€í•œ ì¢…í•©ì ì¸ ê²½í—˜ì„ ìŒ“ì„ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. íŠ¹íˆ, LightGBM, XGBoost, CatBoostì™€ ê°™ì€ ë¶€ìŠ¤íŒ… ëª¨ë¸ì˜ ê°•ë ¥í•¨ì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆê³ , ë°ì´í„° ì²˜ë¦¬ì™€ ëª¨ë¸ ì„ íƒì´ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ê²½í—˜í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ë¶„ë¥˜ ë¬¸ì œëŠ” ë°ì´í„°ì˜ íŠ¹ì„±ê³¼ ëª¨ë¸ì˜ ì„ íƒì´ ì¤‘ìš”í•œ ì—­í• ì„ í•˜ë©°, ì´ë¥¼ ì˜ ì´í•´í•˜ê³  í™œìš©í•˜ëŠ” ê²ƒì´ ì¢‹ì€ ì„±ëŠ¥ì„ ì´ëŒì–´ ë‚´ëŠ” í•µì‹¬ì„ì„ ê¹¨ë‹¬ì•˜ìŠµë‹ˆë‹¤.

ë‹¤ìŒ ê²Œì‹œë¬¼ë¶€í„°ëŠ” íšŒê·€ ë¬¸ì œë¥¼ ë‹¤ë£¨ë©° ìƒˆë¡œìš´ ì¸ì‚¬ì´íŠ¸ë¥¼ ì–»ëŠ” ê³¼ì •ì„ ì§„í–‰í•˜ë ¤ê³  í•©ë‹ˆë‹¤. ì´ë²ˆ ë¶„ë¥˜ ë¬¸ì œë¥¼ í†µí•´ ì–»ì€ ê²½í—˜ì„ ë°”íƒ•ìœ¼ë¡œ ë” ë°œì „ëœ íšŒê·€ ë¶„ì„ì„ ë‹¤ë£° ì˜ˆì •ì´ë©°, ì´ì™€ ê°™ì€ ê³¼ì •ì„ ë°˜ë³µí•˜ë©° ë°ì´í„° ë¶„ì„ ë° ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë§ ì—­ëŸ‰ì„ ì§€ì†ì ìœ¼ë¡œ í–¥ìƒì‹œí‚¬ ê²ƒì…ë‹ˆë‹¤.


